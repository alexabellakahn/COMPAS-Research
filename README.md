# COMPAS-Research
COMPAS Dataset Research

This project analyzes the COMPAS dataset, with the aim of building fairer machine learning models and exploring the ethical tradeoffs involved in technical design decisions. The code begins by downloading the dataset, loading it into a pandas DataFrame, and cleaning it by removing unnecessary columns such as personal identifiers, redundant information, and fields unrelated to our task. The remaining columns are renamed for clarity. Preprocessing steps include filtering out rare charges (charges with fewer than 70 occurrences) and one-hot encoding categorical variables like race, sex, and age categories. This process converts categorical data into binary columns suitable for machine learning models, ensuring the data is ready for analysis.

The project builds multiple models to analyze the dataset. First, a logistic regression model is trained to predict whether a defendant will recidivate within two years, using the preprocessed data. The dataset is split into training and testing sets (70% training, 30% testing), and the model's accuracy is evaluated on both sets to assess its performance. The code also separates the test dataset by racial groups (African-American and Caucasian) to analyze the model's fairness. For each group, confusion matrices are generated to visualize the distribution of true positives, false positives, true negatives, and false negatives. The false positive rates (FPRs) are calculated to determine if one racial group is being disproportionately misclassified.

To address fairness concerns, the code adjusts classification thresholds to equalize FPRs across groups. This involves calculating the FPRs for different thresholds and finding the threshold for African-Americans that minimizes the difference between their FPR and the FPR for Caucasians. After applying the adjusted threshold, the model is re-evaluated to ensure a more equitable distribution of errors between groups. The adjusted model's performance, including its accuracy and fairness metrics, is analyzed and reported.

The project also includes a Support Vector Machine (SVM) model and a Random Forest Classifier to explore the impact of different algorithms on fairness and performance. For each model, feature importances are calculated to understand which variables contribute most to the predictions. These importances are visualized using bar plots, with a specific focus on race-related features to assess their influence on the models' decisions.

Finally, the project includes a neural network model (Multi-Layer Perceptron), which is trained and evaluated in a similar manner. The neural network’s design includes hidden layers and hyperparameters that allow it to learn complex patterns in the data. The model’s performance is compared to the simpler models, emphasizing the tradeoff between complexity and interpretability.

Throughout the project, fairness in machine learning is a central theme. The code demonstrates how to identify bias in model predictions, use threshold adjustments to address fairness concerns, and evaluate fairness using definitions like equalized FPRs. These steps highlight the ethical implications of machine learning design, especially in sensitive applications like the criminal justice system, where biased predictions can have significant real-world consequences. This project serves as a practical guide for understanding, analyzing, and mitigating bias in machine learning models, while balancing performance and fairness.
